{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "def cosine_sim(v1,v2): return 1 - spatial.distance.cosine(v1,v2)\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_1.csv')\n",
    "train2 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_2.csv')\n",
    "train = pd.concat([train1, train2])\n",
    "train = train[train.notnull().apply(all, axis=1)]\n",
    "dev = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_dev.csv')\n",
    "dev = dev[dev.notnull().apply(all, axis=1)]\n",
    "test = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_test.csv')\n",
    "test = test[test.notnull().apply(all, axis=1)]\n",
    "\n",
    "dev_prepared = pd.read_csv('../02_Extract_Subphrases/prepared_data/subphrase_vectors_dev.csv', sep=';')\n",
    "dev_prepared = dev_prepared.drop(columns='Unnamed: 0')\n",
    "dev = dev.set_index('pairID')\n",
    "rel_pairIDs = dev_prepared.iloc[:,0]\n",
    "y_hat = dev.loc[rel_pairIDs].gold_label\n",
    "dev_prepared = dev_prepared.iloc[:,1:].to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "BN_dev_small = pd.read_csv(\"../04_BN_Explanations/BN_explanations_small_model.csv\", sep=\";\", index_col=0)\n",
    "BN_dev_large = pd.read_csv(\"../04_BN_Explanations/BN_explanations_large_model.csv\", sep=\";\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "dev = dev.loc[rel_pairIDs]\n",
    "dev = dev.iloc[BN_dev_large.i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "temp = [None, ] * 10\n",
    "for i in range(1, 11):\n",
    "    temp[i - 1] = pd.read_csv(\"../01_GPT3_Explanations/prepared_data/GPT3_explanations\" + str(i) + \".csv\", sep=\";\")\n",
    "gpt3_dev = pd.concat(temp).set_index(\"pairID\")\n",
    "gpt3_dev = gpt3_dev.loc[rel_pairIDs]\n",
    "gpt3_dev = gpt3_dev.iloc[BN_dev_large.i].reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "all_explanations = gpt3_dev.pred_explanation.to_list() + gpt3_dev.gold_standard_explanation.to_list() + BN_dev_large.BN_expl.to_list() + BN_dev_small.BN_expl.to_list()\n",
    "binary_counts = vectorizer.fit_transform(all_explanations)\n",
    "all_models_binary = binary_counts.toarray()\n",
    "gpt3_binary = all_models_binary[:gpt3_dev.shape[0]]\n",
    "gold_binary = all_models_binary[gpt3_dev.shape[0]:2*gpt3_dev.shape[0]]\n",
    "ssm_large_binary = all_models_binary[2*gpt3_dev.shape[0]:2*gpt3_dev.shape[0]+BN_dev_large.shape[0]]\n",
    "ssm_small_binary = all_models_binary[-BN_dev_small.shape[0]:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "jaccard_scores_ssm_small = [jaccard_score(explanation_ssm_small, explanation_gpt3) for explanation_ssm_small, explanation_gpt3 in zip(ssm_small_binary, gpt3_binary)]\n",
    "jaccard_scores_ssm_large = [jaccard_score(explanation_ssm_large, explanation_gpt3) for explanation_ssm_large, explanation_gpt3 in zip(ssm_large_binary, gpt3_binary)]\n",
    "jaccard_scores_gold = [jaccard_score(explanation_gold, explanation_gpt3) for explanation_gold, explanation_gpt3 in zip(gold_binary, gpt3_binary)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "embedding_vecs_gpt3 = [nlp(s).vector for s in gpt3_dev.pred_explanation]\n",
    "embedding_vecs_gold = [nlp(s).vector for s in gpt3_dev.gold_standard_explanation]\n",
    "embedding_vecs_SSM_small = [nlp(s).vector for s in BN_dev_small.BN_expl]\n",
    "embedding_vecs_SSM_large = [nlp(s).vector for s in BN_dev_large.BN_expl]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "cosine_scores_ssm_small = [cosine_sim(explanation_ssm_small, explanation_gpt3) for explanation_ssm_small, explanation_gpt3 in zip(embedding_vecs_SSM_small, embedding_vecs_gpt3)]\n",
    "cosine_scores_ssm_large = [cosine_sim(explanation_ssm_large, explanation_gpt3) for explanation_ssm_large, explanation_gpt3 in zip(embedding_vecs_SSM_large, embedding_vecs_gpt3)]\n",
    "cosine_scores_gold = [cosine_sim(explanation_gold, explanation_gpt3) for explanation_gold, explanation_gpt3 in zip(embedding_vecs_gold, embedding_vecs_gpt3)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_scores_precision_ssm_small, bert_scores_recall_ssm_small, bert_scores_f1_ssm_small = score(BN_dev_small.BN_expl.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "bert_scores_precision_ssm_large, bert_scores_recall_ssm_large, bert_scores_f1_ssm_large = score(BN_dev_large.BN_expl.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "bert_scores_precision_gold, bert_scores_recall_gold, bert_scores_f1_gold = score(gpt3_dev.gold_standard_explanation.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "jaccard_scores_ssm_small = np.array(jaccard_scores_ssm_small)\n",
    "jaccard_scores_ssm_large = np.array(jaccard_scores_ssm_large)\n",
    "jaccard_scores_gold = np.array(jaccard_scores_gold)\n",
    "\n",
    "cosine_scores_ssm_small = np.array(cosine_scores_ssm_small)\n",
    "cosine_scores_ssm_large = np.array(cosine_scores_ssm_large)\n",
    "cosine_scores_gold = np.array(cosine_scores_gold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "index_correct_small_SSM_preds = np.where(BN_dev_small.y_hat_BN == gpt3_dev.pred_label)[0]\n",
    "index_incorrect_small_SSM_preds = np.where(BN_dev_small.y_hat_BN != gpt3_dev.pred_label)[0]\n",
    "index_correct_large_SSM_preds = np.where(BN_dev_large.y_hat_BN == gpt3_dev.pred_label)[0]\n",
    "index_incorrect_large_SSM_preds = np.where(BN_dev_large.y_hat_BN != gpt3_dev.pred_label)[0]\n",
    "index_correct_gpt3_preds = np.where(gpt3_dev.pred_label == gpt3_dev.gold_standard_label)[0]\n",
    "index_incorrect_gpt3_preds = np.where(gpt3_dev.pred_label != gpt3_dev.gold_standard_label)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between GPT-3 explanations and small SSM: 0.196, 0.107 (0.218, 0.115 / 0.168, 0.087)\n",
      "Jaccard similarity between GPT-3 explanations and large SSM: 0.182, 0.095 (0.198, 0.102 / 0.168, 0.087)\n",
      "Jaccard similarity between GPT-3 explanations and gold explanations: 0.277, 0.158 (0.288, 0.162 / 0.233, 0.134)\n",
      "==============================================================================\n",
      "Cosine similarity between GPT-3 explanations and small SSM: 0.779, 0.089 (0.793, 0.087 / 0.76, 0.088)\n",
      "Cosine similarity between GPT-3 explanations and large SSM: 0.771, 0.09 (0.787, 0.087 / 0.756, 0.089)\n",
      "Cosine similarity between GPT-3 explanations and gold explanations: 0.808, 0.116 (0.811, 0.117 / 0.793, 0.11)\n",
      "==============================================================================\n",
      "BERTScore between GPT-3 explanations and small SSM: 0.46299999952316284, 0.0689999982714653 (0.4790000021457672, 0.07000000029802322 / 0.4440000057220459, 0.06199999898672104)\n",
      "BERTScore between GPT-3 explanations and large SSM: 0.45500001311302185, 0.06300000101327896 (0.4690000116825104, 0.06199999898672104 / 0.4429999887943268, 0.061000000685453415)\n",
      "BERTScore between GPT-3 explanations and gold explanations: 0.6039999723434448, 0.10300000011920929 (0.609000027179718, 0.10499999672174454 / 0.5799999833106995, 0.09099999815225601)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jaccard similarity between GPT-3 explanations and small SSM: {round(np.mean(jaccard_scores_ssm_small), 3)}, {round(np.std(jaccard_scores_ssm_small), 3)} ({round(np.mean(jaccard_scores_ssm_small[index_correct_small_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_small[index_correct_small_SSM_preds]), 3)} / {round(np.mean(jaccard_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Jaccard similarity between GPT-3 explanations and large SSM: {round(np.mean(jaccard_scores_ssm_large), 3)}, {round(np.std(jaccard_scores_ssm_large), 3)} ({round(np.mean(jaccard_scores_ssm_large[index_correct_large_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_large[index_correct_large_SSM_preds]), 3)} / {round(np.mean(jaccard_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Jaccard similarity between GPT-3 explanations and gold explanations: {round(np.mean(jaccard_scores_gold), 3)}, {round(np.std(jaccard_scores_gold), 3)} ({round(np.mean(jaccard_scores_gold[index_correct_gpt3_preds]), 3)}, {round(np.std(jaccard_scores_gold[index_correct_gpt3_preds]), 3)} / {round(np.mean(jaccard_scores_gold[index_incorrect_gpt3_preds]), 3)}, {round(np.std(jaccard_scores_gold[index_incorrect_gpt3_preds]), 3)})\")\n",
    "\n",
    "print(\"==============================================================================\")\n",
    "print(f\"Cosine similarity between GPT-3 explanations and small SSM: {round(np.mean(cosine_scores_ssm_small), 3)}, {round(np.std(cosine_scores_ssm_small), 3)} ({round(np.mean(cosine_scores_ssm_small[index_correct_small_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_small[index_correct_small_SSM_preds]), 3)} / {round(np.mean(cosine_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Cosine similarity between GPT-3 explanations and large SSM: {round(np.mean(cosine_scores_ssm_large), 3)}, {round(np.std(cosine_scores_ssm_large), 3)} ({round(np.mean(cosine_scores_ssm_large[index_correct_large_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_large[index_correct_large_SSM_preds]), 3)} / {round(np.mean(cosine_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Cosine similarity between GPT-3 explanations and gold explanations: {round(np.mean(cosine_scores_gold), 3)}, {round(np.std(cosine_scores_gold), 3)} ({round(np.mean(cosine_scores_gold[index_correct_gpt3_preds]), 3)}, {round(np.std(cosine_scores_gold[index_correct_gpt3_preds]), 3)} / {round(np.mean(cosine_scores_gold[index_incorrect_gpt3_preds]), 3)}, {round(np.std(cosine_scores_gold[index_incorrect_gpt3_preds]), 3)})\")\n",
    "\n",
    "print(\"==============================================================================\")\n",
    "print(f\"BERTScore between GPT-3 explanations and small SSM: {round(np.mean(bert_scores_f1_ssm_small.numpy()), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()), 3)} ({round(np.mean(bert_scores_f1_ssm_small.numpy()[index_correct_small_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()[index_correct_small_SSM_preds]), 3)} / {round(np.mean(bert_scores_f1_ssm_small.numpy()[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"BERTScore between GPT-3 explanations and large SSM: {round(np.mean(bert_scores_f1_ssm_large.numpy()), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()), 3)} ({round(np.mean(bert_scores_f1_ssm_large.numpy()[index_correct_large_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()[index_correct_large_SSM_preds]), 3)} / {round(np.mean(bert_scores_f1_ssm_large.numpy()[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"BERTScore between GPT-3 explanations and gold explanations: {round(np.mean(bert_scores_f1_gold.numpy()), 3)}, {round(np.std(bert_scores_f1_gold.numpy()), 3)} ({round(np.mean(bert_scores_f1_gold.numpy()[index_correct_gpt3_preds]), 3)}, {round(np.std(bert_scores_f1_gold.numpy()[index_correct_gpt3_preds]), 3)} / {round(np.mean(bert_scores_f1_gold.numpy()[index_incorrect_gpt3_preds]), 3)}, {round(np.std(bert_scores_f1_gold.numpy()[index_incorrect_gpt3_preds]), 3)})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}