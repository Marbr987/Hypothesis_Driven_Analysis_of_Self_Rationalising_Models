{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "def cosine_sim(v1,v2): return 1 - spatial.distance.cosine(v1,v2)\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_1.csv')\n",
    "train2 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_2.csv')\n",
    "train = pd.concat([train1, train2])\n",
    "train = train[train.notnull().apply(all, axis=1)]\n",
    "dev = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_dev.csv')\n",
    "dev = dev[dev.notnull().apply(all, axis=1)]\n",
    "test = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_test.csv')\n",
    "test = test[test.notnull().apply(all, axis=1)]\n",
    "\n",
    "dev_prepared = pd.read_csv('../02_Extract_Subphrases/prepared_data/subphrase_vectors_dev.csv', sep=';')\n",
    "dev_prepared = dev_prepared.drop(columns='Unnamed: 0')\n",
    "dev = dev.set_index('pairID')\n",
    "rel_pairIDs = dev_prepared.iloc[:,0]\n",
    "y_hat = dev.loc[rel_pairIDs].gold_label\n",
    "dev_prepared = dev_prepared.iloc[:,1:].to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BN_dev_small = pd.read_csv(\"../04_BN_Explanations/BN_explanations_small_model.csv\", sep=\";\", index_col=0)\n",
    "BN_dev_large = pd.read_csv(\"../04_BN_Explanations/BN_explanations_large_model.csv\", sep=\";\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dev = dev.loc[rel_pairIDs]\n",
    "dev = dev.iloc[BN_dev_large.i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "temp = [None, ] * 10\n",
    "for i in range(1, 11):\n",
    "    temp[i - 1] = pd.read_csv(\"../01_GPT3_Explanations/prepared_data/GPT3_explanations\" + str(i) + \".csv\", sep=\";\")\n",
    "gpt3_dev = pd.concat(temp).set_index(\"pairID\")\n",
    "gpt3_dev = gpt3_dev.loc[rel_pairIDs]\n",
    "gpt3_dev = gpt3_dev.iloc[BN_dev_large.i].reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "all_explanations = gpt3_dev.pred_explanation.to_list() + gpt3_dev.gold_standard_explanation.to_list() + BN_dev_large.BN_expl.to_list() + BN_dev_small.BN_expl.to_list()\n",
    "binary_counts = vectorizer.fit_transform(all_explanations)\n",
    "all_models_binary = binary_counts.toarray()\n",
    "gpt3_binary = all_models_binary[:gpt3_dev.shape[0]]\n",
    "gold_binary = all_models_binary[gpt3_dev.shape[0]:2*gpt3_dev.shape[0]]\n",
    "ssm_large_binary = all_models_binary[2*gpt3_dev.shape[0]:2*gpt3_dev.shape[0]+BN_dev_large.shape[0]]\n",
    "ssm_small_binary = all_models_binary[-BN_dev_small.shape[0]:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "jaccard_scores_ssm_small = [jaccard_score(explanation_ssm_small, explanation_gpt3) for explanation_ssm_small, explanation_gpt3 in zip(ssm_small_binary, gpt3_binary)]\n",
    "jaccard_scores_ssm_large = [jaccard_score(explanation_ssm_large, explanation_gpt3) for explanation_ssm_large, explanation_gpt3 in zip(ssm_large_binary, gpt3_binary)]\n",
    "jaccard_scores_gold = [jaccard_score(explanation_gold, explanation_gpt3) for explanation_gold, explanation_gpt3 in zip(gold_binary, gpt3_binary)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "embedding_vecs_gpt3 = [nlp(s).vector for s in gpt3_dev.pred_explanation]\n",
    "embedding_vecs_gold = [nlp(s).vector for s in gpt3_dev.gold_standard_explanation]\n",
    "embedding_vecs_SSM_small = [nlp(s).vector for s in BN_dev_small.BN_expl]\n",
    "embedding_vecs_SSM_large = [nlp(s).vector for s in BN_dev_large.BN_expl]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cosine_scores_ssm_small = [cosine_sim(explanation_ssm_small, explanation_gpt3) for explanation_ssm_small, explanation_gpt3 in zip(embedding_vecs_SSM_small, embedding_vecs_gpt3)]\n",
    "cosine_scores_ssm_large = [cosine_sim(explanation_ssm_large, explanation_gpt3) for explanation_ssm_large, explanation_gpt3 in zip(embedding_vecs_SSM_large, embedding_vecs_gpt3)]\n",
    "cosine_scores_gold = [cosine_sim(explanation_gold, explanation_gpt3) for explanation_gold, explanation_gpt3 in zip(embedding_vecs_gold, embedding_vecs_gpt3)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_scores_precision_ssm_small, bert_scores_recall_ssm_small, bert_scores_f1_ssm_small = score(BN_dev_small.BN_expl.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "bert_scores_precision_ssm_large, bert_scores_recall_ssm_large, bert_scores_f1_ssm_large = score(BN_dev_large.BN_expl.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "bert_scores_precision_gold, bert_scores_recall_gold, bert_scores_f1_gold = score(gpt3_dev.gold_standard_explanation.to_list(), gpt3_dev.pred_explanation.to_list(), lang=\"en\", model_type=\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "jaccard_scores_ssm_small = np.array(jaccard_scores_ssm_small)\n",
    "jaccard_scores_ssm_large = np.array(jaccard_scores_ssm_large)\n",
    "jaccard_scores_gold = np.array(jaccard_scores_gold)\n",
    "\n",
    "cosine_scores_ssm_small = np.array(cosine_scores_ssm_small)\n",
    "cosine_scores_ssm_large = np.array(cosine_scores_ssm_large)\n",
    "cosine_scores_gold = np.array(cosine_scores_gold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "index_correct_small_SSM_preds = np.where(BN_dev_small.y_hat_BN == gpt3_dev.pred_label)[0]\n",
    "index_incorrect_small_SSM_preds = np.where(BN_dev_small.y_hat_BN != gpt3_dev.pred_label)[0]\n",
    "index_correct_large_SSM_preds = np.where(BN_dev_large.y_hat_BN == gpt3_dev.pred_label)[0]\n",
    "index_incorrect_large_SSM_preds = np.where(BN_dev_large.y_hat_BN != gpt3_dev.pred_label)[0]\n",
    "index_correct_gpt3_preds = np.where(gpt3_dev.pred_label == gpt3_dev.gold_standard_label)[0]\n",
    "index_incorrect_gpt3_preds = np.where(gpt3_dev.pred_label != gpt3_dev.gold_standard_label)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between GPT-3 explanations and small SSM: 0.196, 0.107 (0.218, 0.115 / 0.168, 0.087)\n",
      "Jaccard similarity between GPT-3 explanations and large SSM: 0.182, 0.095 (0.198, 0.102 / 0.168, 0.087)\n",
      "Jaccard similarity between GPT-3 explanations and gold explanations: 0.277, 0.158 (0.288, 0.162 / 0.233, 0.134)\n",
      "==============================================================================\n",
      "Cosine similarity between GPT-3 explanations and small SSM: 0.779, 0.089 (0.793, 0.087 / 0.76, 0.088)\n",
      "Cosine similarity between GPT-3 explanations and large SSM: 0.771, 0.09 (0.787, 0.087 / 0.756, 0.089)\n",
      "Cosine similarity between GPT-3 explanations and gold explanations: 0.808, 0.116 (0.811, 0.117 / 0.793, 0.11)\n",
      "==============================================================================\n",
      "BERTScore between GPT-3 explanations and small SSM: 0.46299999952316284, 0.0689999982714653 (0.4790000021457672, 0.07000000029802322 / 0.4440000057220459, 0.06199999898672104)\n",
      "BERTScore between GPT-3 explanations and large SSM: 0.45500001311302185, 0.06300000101327896 (0.4690000116825104, 0.06199999898672104 / 0.4429999887943268, 0.061000000685453415)\n",
      "BERTScore between GPT-3 explanations and gold explanations: 0.6039999723434448, 0.10300000011920929 (0.609000027179718, 0.10499999672174454 / 0.5799999833106995, 0.09099999815225601)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jaccard similarity between GPT-3 explanations and small SSM: {round(np.mean(jaccard_scores_ssm_small), 3)}, {round(np.std(jaccard_scores_ssm_small), 3)} ({round(np.mean(jaccard_scores_ssm_small[index_correct_small_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_small[index_correct_small_SSM_preds]), 3)} / {round(np.mean(jaccard_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Jaccard similarity between GPT-3 explanations and large SSM: {round(np.mean(jaccard_scores_ssm_large), 3)}, {round(np.std(jaccard_scores_ssm_large), 3)} ({round(np.mean(jaccard_scores_ssm_large[index_correct_large_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_large[index_correct_large_SSM_preds]), 3)} / {round(np.mean(jaccard_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(jaccard_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Jaccard similarity between GPT-3 explanations and gold explanations: {round(np.mean(jaccard_scores_gold), 3)}, {round(np.std(jaccard_scores_gold), 3)} ({round(np.mean(jaccard_scores_gold[index_correct_gpt3_preds]), 3)}, {round(np.std(jaccard_scores_gold[index_correct_gpt3_preds]), 3)} / {round(np.mean(jaccard_scores_gold[index_incorrect_gpt3_preds]), 3)}, {round(np.std(jaccard_scores_gold[index_incorrect_gpt3_preds]), 3)})\")\n",
    "\n",
    "print(\"==============================================================================\")\n",
    "print(f\"Cosine similarity between GPT-3 explanations and small SSM: {round(np.mean(cosine_scores_ssm_small), 3)}, {round(np.std(cosine_scores_ssm_small), 3)} ({round(np.mean(cosine_scores_ssm_small[index_correct_small_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_small[index_correct_small_SSM_preds]), 3)} / {round(np.mean(cosine_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_small[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Cosine similarity between GPT-3 explanations and large SSM: {round(np.mean(cosine_scores_ssm_large), 3)}, {round(np.std(cosine_scores_ssm_large), 3)} ({round(np.mean(cosine_scores_ssm_large[index_correct_large_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_large[index_correct_large_SSM_preds]), 3)} / {round(np.mean(cosine_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(cosine_scores_ssm_large[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"Cosine similarity between GPT-3 explanations and gold explanations: {round(np.mean(cosine_scores_gold), 3)}, {round(np.std(cosine_scores_gold), 3)} ({round(np.mean(cosine_scores_gold[index_correct_gpt3_preds]), 3)}, {round(np.std(cosine_scores_gold[index_correct_gpt3_preds]), 3)} / {round(np.mean(cosine_scores_gold[index_incorrect_gpt3_preds]), 3)}, {round(np.std(cosine_scores_gold[index_incorrect_gpt3_preds]), 3)})\")\n",
    "\n",
    "print(\"==============================================================================\")\n",
    "print(f\"BERTScore between GPT-3 explanations and small SSM: {round(np.mean(bert_scores_f1_ssm_small.numpy()), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()), 3)} ({round(np.mean(bert_scores_f1_ssm_small.numpy()[index_correct_small_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()[index_correct_small_SSM_preds]), 3)} / {round(np.mean(bert_scores_f1_ssm_small.numpy()[index_incorrect_small_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_small.numpy()[index_incorrect_small_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"BERTScore between GPT-3 explanations and large SSM: {round(np.mean(bert_scores_f1_ssm_large.numpy()), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()), 3)} ({round(np.mean(bert_scores_f1_ssm_large.numpy()[index_correct_large_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()[index_correct_large_SSM_preds]), 3)} / {round(np.mean(bert_scores_f1_ssm_large.numpy()[index_incorrect_large_SSM_preds]), 3)}, {round(np.std(bert_scores_f1_ssm_large.numpy()[index_incorrect_large_SSM_preds]), 3)})\")\n",
    "\n",
    "print(f\"BERTScore between GPT-3 explanations and gold explanations: {round(np.mean(bert_scores_f1_gold.numpy()), 3)}, {round(np.std(bert_scores_f1_gold.numpy()), 3)} ({round(np.mean(bert_scores_f1_gold.numpy()[index_correct_gpt3_preds]), 3)}, {round(np.std(bert_scores_f1_gold.numpy()[index_correct_gpt3_preds]), 3)} / {round(np.mean(bert_scores_f1_gold.numpy()[index_incorrect_gpt3_preds]), 3)}, {round(np.std(bert_scores_f1_gold.numpy()[index_incorrect_gpt3_preds]), 3)})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate factually correct explanations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "h1 = pd.read_excel(\"evaluation_template_h1.xlsx\")\n",
    "use_indices = ~h1.subphrase_correctness_SSM_small.isnull()\n",
    "h1 = h1[use_indices]\n",
    "h1 = h1.drop(columns=h1.columns[11:21]).fillna(0)\n",
    "\n",
    "h2 = pd.read_excel(\"evaluation_template_h2.xlsx\")\n",
    "h2 = h2[use_indices]\n",
    "h2 = h2.drop(columns=h2.columns[11:21]).fillna(0)\n",
    "\n",
    "h3 = pd.read_excel(\"evaluation_template_h3.xlsx\")\n",
    "h3 = h3[use_indices]\n",
    "h3 = h3.drop(columns=h3.columns[11:21]).fillna(0)\n",
    "train1 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_1.csv')\n",
    "train2 = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_train_2.csv')\n",
    "train = pd.concat([train1, train2])\n",
    "train = train[train.notnull().apply(all, axis=1)]\n",
    "dev = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_dev.csv')\n",
    "dev = dev[dev.notnull().apply(all, axis=1)]\n",
    "test = pd.read_csv('../Input_Data/e-SNLI/dataset/esnli_test.csv')\n",
    "test = test[test.notnull().apply(all, axis=1)]\n",
    "\n",
    "dev_prepared = pd.read_csv('../02_Extract_Subphrases/prepared_data/subphrase_vectors_dev.csv', sep=';')\n",
    "dev_prepared = dev_prepared.drop(columns='Unnamed: 0')\n",
    "dev = dev.set_index('pairID')\n",
    "rel_pairIDs = dev_prepared.iloc[:, 0]\n",
    "y_hat = dev.loc[rel_pairIDs].gold_label\n",
    "dev_prepared = dev_prepared.iloc[:, 1:].to_numpy()\n",
    "dev_prepared = dev_prepared[h1.i]\n",
    "BN_dev_small = pd.read_csv(\"../04_BN_Explanations/BN_explanations_small_model.csv\", sep=\";\", index_col=0)\n",
    "BN_dev_small = BN_dev_small.iloc[h3.i].reset_index()\n",
    "BN_dev_large = pd.read_csv(\"../04_BN_Explanations/BN_explanations_large_model.csv\", sep=\";\", index_col=0)\n",
    "BN_dev_large = BN_dev_large.iloc[h3.i].reset_index()\n",
    "dev = dev.loc[rel_pairIDs]\n",
    "dev = dev.iloc[BN_dev_large.i]\n",
    "temp = [None, ] * 10\n",
    "for i in range(1, 11):\n",
    "    temp[i - 1] = pd.read_csv(\"../01_GPT3_Explanations/prepared_data/GPT3_explanations\" + str(i) + \".csv\", sep=\";\")\n",
    "gpt3_dev = pd.concat(temp).set_index(\"pairID\")\n",
    "gpt3_dev = gpt3_dev.loc[rel_pairIDs]\n",
    "gpt3_dev = gpt3_dev.iloc[h3.i].reset_index()\n",
    "dev_subphrases = pd.read_csv('../02_Extract_Subphrases/prepared_data/subphrases_dev.csv', sep=',')\n",
    "dev_subphrases = dev_subphrases.set_index('pairID')\n",
    "dev_subphrases = dev_subphrases.loc[rel_pairIDs]\n",
    "dev_subphrases = dev_subphrases.drop(\"Unnamed: 0\", axis=1)\n",
    "dev_subphrases = dev_subphrases.iloc[h3.i]\n",
    "h = h1[h1.columns[11:]] + h2[h2.columns[11:]] + h3[h3.columns[11:]]\n",
    "h = h.applymap(lambda x: 1 if x >= 2 else 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 0,  7, 11, 12, 17, 18, 22, 27, 29, 37, 44, 47, 49, 54, 57, 62, 63,\n        67, 73, 77, 80, 84, 85, 90, 91, 94]),)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(h.full_correctness_SSM_small)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "pairID\n5971287030.jpg#0r1c    contradiction\n3670918456.jpg#3r1n          neutral\n4873970424.jpg#0r1e       entailment\n4687453573.jpg#3r1e          neutral\n4727548713.jpg#2r1c    contradiction\n560278886.jpg#1r1e        entailment\n3826467863.jpg#3r1c    contradiction\n2676764246.jpg#0r1e       entailment\n482642539.jpg#2r1e        entailment\n3735771637.jpg#2r1c    contradiction\n3757332635.jpg#2r1c    contradiction\n3394070357.jpg#2r1c    contradiction\n4633788691.jpg#0r1e       entailment\n5075833333.jpg#3r1e       entailment\n1536597926.jpg#0r1n          neutral\n2634085089.jpg#0r1c    contradiction\n5558170783.jpg#1r1n          neutral\n466956209.jpg#1r1c     contradiction\n86800579.jpg#1r1c      contradiction\n3038760935.jpg#3r1n          neutral\n7162943359.jpg#4r1n          neutral\n2309327462.jpg#4r1e       entailment\n2696129516.jpg#3r1c    contradiction\n2766630484.jpg#3r1c    contradiction\n3440104178.jpg#0r1n          neutral\n8217001488.jpg#3r1c    contradiction\nName: gold_label, dtype: object"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.iloc[np.where(h.full_correctness_SSM_small)[0]].gold_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6538461538461539"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(BN_dev_small.iloc[np.where(h.full_correctness_SSM_small)[0]].y == BN_dev_small.iloc[np.where(h.full_correctness_SSM_small)[0]].y_hat_BN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}